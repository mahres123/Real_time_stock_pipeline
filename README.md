# ğŸ“ˆ Real-Time Stock Data Pipeline 

Ce projet vise Ã  construire une architecture de traitement **temps rÃ©el** pour lâ€™analyse de donnÃ©es boursiÃ¨res, combinant **streaming Kafka**, **indexation Elasticsearch**, **visualisation Kibana**, et **modÃ©lisation ML avec BigQuery sur GCP**.

---

## ğŸ¯ Objectif

- Collecter en temps rÃ©el des **flux de donnÃ©es boursiÃ¨res simulÃ©es**
- Les **consommer et indexer** dans Elasticsearch pour la visualisation immÃ©diate
- Les **stocker dans Google Cloud Storage** pour des traitements batch/ML
- EntraÃ®ner des **modÃ¨les de prÃ©diction supervisÃ©e** (rÃ©gression linÃ©aire, sÃ©ries temporelles)
- Construire un **dashboard Kibana interactif**

---

## ğŸ§± Architecture technique
producer.py â”€â”€â”€â–¶ Kafka â”€â”€â”€â–¶ consumer_to_elasticsearch.py â”€â”€â–¶ Elasticsearch
â”‚
â–¼
Google Cloud Storage
â”‚
â–¼
BigQuery ML Models

---

## ğŸ—‚ Structure du projet

Real_time_stock_pipeline/
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py                    # Envoie les messages JSON dans Kafka
â”‚   â”œâ”€â”€ consumer.py                    # Consumer simple (debug terminal)
â”‚   â””â”€â”€ consumer_to_elasticsearch.py   # Consomme Kafka â†’ indexe dans Elasticsearch
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ stock_data_sample.json         # DonnÃ©es simulÃ©es envoyÃ©es dans Kafka
â”‚   â””â”€â”€ README.md                      # Instructions pour log_inf.csv
â”‚
â”œâ”€â”€ elasticsearch_kibana/
â”‚   â””â”€â”€ mapping_stock_stream.json      # Mapping personnalisÃ© pour Elasticsearch
â”‚
â”œâ”€â”€ gcp_bigquery/
â”‚   â”œâ”€â”€ upload_to_gcs.py                            # Upload JSON vers GCS
â”‚   â”œâ”€â”€ create_external_table.sql                   # Table externe BigQuery
â”‚   â”œâ”€â”€ train_price_regression_model.sql            # RÃ©gression linÃ©aire
â”‚   â”œâ”€â”€ predict_price.sql                           # PrÃ©diction du last_price
â”‚   â”œâ”€â”€ train_and_predict_timeseries.sql            # ModÃ¨le ARIMA simple
â”‚   â””â”€â”€ train_and_predict_timeseries_multiseries.sql # ARIMA multi-instruments
â”‚
â”œâ”€â”€ docker-compose.yml                # Conteneurisation de Kafka, ES, Kibana
â””â”€â”€ README.md                         # Documentation du projet
---

## âš™ï¸ Fonctionnement Ã©tape par Ã©tape

### 1. Kafka Streaming (Python)

- `producer.py` lit un fichier JSON simulÃ© et envoie des messages sur le topic `stock-stream`
- `consumer_to_elasticsearch.py` consomme ce flux et lâ€™indexe en direct dans Elasticsearch

### 2. Elasticsearch + Kibana

- Un **mapping personnalisÃ©** (`mapping_stock_stream.json`) est appliquÃ© via cURL :

```bash
curl -X PUT http://localhost:9200/_index_template/stock-stream-template \
  -H "Content-Type: application/json" \
  -d @elasticsearch_kibana/mapping_stock_stream.json 

### 3. Google Cloud Platform (GCS + BigQuery ML)

#### a. ğŸ“¦ Stockage dans GCS
- Le fichier `stock_data_sample.json` est uploadÃ© via `upload_to_gcs.py`
- Le bucket est accessible dans BigQuery

#### b. ğŸ—ƒ Table externe BigQuery
- DÃ©finie dans `create_external_table.sql`
- ConnectÃ©e directement au fichier JSON sur GCS

#### c. ğŸ§  ModÃ©lisation ML

âœ… **RÃ©gression linÃ©aire** :
- `train_price_regression_model.sql` crÃ©e un modÃ¨le supervisÃ© pour prÃ©dire `last_price`
- `predict_price.sql` gÃ©nÃ¨re des prÃ©dictions

âœ… **SÃ©ries temporelles (ARIMA)** :
- `train_and_predict_timeseries.sql` : prÃ©diction simple basÃ©e sur `timestamp`
- `train_and_predict_timeseries_multiseries.sql` : modÃ¨le multi-instruments (via `instrument_token`)


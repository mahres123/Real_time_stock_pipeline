# ğŸ“ˆ Real-Time Stock Data Pipeline with Apache Kafka on GCP

Ce projet vise Ã  construire une architecture de traitement **temps rÃ©el** pour lâ€™analyse de donnÃ©es boursiÃ¨res, combinant **streaming Kafka**, **indexation Elasticsearch**, **visualisation Kibana**, et **modÃ©lisation ML avec BigQuery sur GCP**.

---

## ğŸ¯ Objectif

- Collecter en temps rÃ©el des **flux de donnÃ©es boursiÃ¨res simulÃ©es**
- Les **consommer et indexer** dans Elasticsearch pour la visualisation immÃ©diate
- Les **stocker dans Google Cloud Storage** pour des traitements batch/ML
- EntraÃ®ner des **modÃ¨les de prÃ©diction supervisÃ©e** (rÃ©gression linÃ©aire, sÃ©ries temporelles)
- Construire un **dashboard Kibana interactif**

---

## ğŸ§± Architecture technique
producer.py â”€â”€â”€â–¶ Kafka â”€â”€â”€â–¶ consumer_to_elasticsearch.py â”€â”€â–¶ Elasticsearch
â”‚
â–¼
Google Cloud Storage
â”‚
â–¼
BigQuery ML Models

---

### ğŸ“ Structure du projet

```text
Real_time_stock_pipeline/
â”œâ”€â”€ kafka/                             # Scripts Kafka (producteur / consommateurs)
â”‚   â”œâ”€â”€ producer.py                    # Envoie les messages JSON dans Kafka
â”‚   â”œâ”€â”€ consumer.py                    # Consommateur simple (terminal)
â”‚   â””â”€â”€ consumer_to_elasticsearch.py   # Consomme Kafka â†’ indexe dans Elasticsearch
â”‚
â”œâ”€â”€ data/                              # DonnÃ©es JSON simulÃ©es utilisÃ©es dans Kafka
â”‚   â””â”€â”€ stock_data_sample.json
â”‚
â”œâ”€â”€ elasticsearch_kibana/              # Configuration pour Elasticsearch + mapping
â”‚   â””â”€â”€ mapping_stock_stream.json
â”‚
â”œâ”€â”€ gcp_bigquery/                      # Partie GCP : GCS + BigQuery + ML
â”‚   â”œâ”€â”€ upload_to_gcs.py
â”‚   â”œâ”€â”€ create_external_table.sql
â”‚   â”œâ”€â”€ train_price_regression_model.sql
â”‚   â”œâ”€â”€ predict_price.sql
â”‚   â”œâ”€â”€ train_and_predict_timeseries.sql
â”‚   â””â”€â”€ train_and_predict_timeseries_multiseries.sql
â”‚
â”œâ”€â”€ docker-compose.yml                 # Lancement de Kafka, Elasticsearch, Kibana
â””â”€â”€ README.md                          # Documentation du projet
```
## âš™ï¸ Fonctionnement Ã©tape par Ã©tape

### 1. Kafka Streaming (Python)

- `producer.py` lit un fichier JSON simulÃ© et envoie des messages sur le topic `stock-stream`
- `consumer_to_elasticsearch.py` consomme ce flux et lâ€™indexe en direct dans Elasticsearch

### 2. Elasticsearch + Kibana 3.Google Cloud Platform (GCS + BigQuery ML)

- Un **mapping personnalisÃ©** (`mapping_stock_stream.json`) est appliquÃ© via cURL :

```bash
curl -X PUT http://localhost:9200/_index_template/stock-stream-template \
  -H "Content-Type: application/json" \
  -d @elasticsearch_kibana/mapping_stock_stream.json 

```

### 3.Google Cloud Platform (GCS + BigQuery ML)


#### a. ğŸ“¦ Stockage dans GCS
- Le fichier `stock_data_sample.json` est uploadÃ© via `upload_to_gcs.py`
- Le bucket est accessible dans BigQuery

#### b. ğŸ—ƒ Table externe BigQuery
- DÃ©finie dans `create_external_table.sql`
- ConnectÃ©e directement au fichier JSON sur GCS

#### c. ğŸ§  ModÃ©lisation ML

âœ… **RÃ©gression linÃ©aire** :
- `train_price_regression_model.sql` crÃ©e un modÃ¨le supervisÃ© pour prÃ©dire `last_price`
- `predict_price.sql` gÃ©nÃ¨re des prÃ©dictions

âœ… **SÃ©ries temporelles (ARIMA)** :
- `train_and_predict_timeseries.sql` : prÃ©diction simple basÃ©e sur `timestamp`
- `train_and_predict_timeseries_multiseries.sql` : modÃ¨le multi-instruments (via `instrument_token`)

